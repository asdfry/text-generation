{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np8-bs32\n",
      "  cpu_op\n",
      "    [1] autograd::engine::evaluate_function: DivBackward0 --> 104us\n",
      "    [2] DivBackward0 --> 82us\n",
      "    [3] aten::div --> 186us\n",
      "    [4] autograd::engine::evaluate_function: NllLossBackward0 --> 176us\n",
      "    [5] NllLossBackward0 --> 145us\n",
      "    [6] aten::nll_loss_backward --> 137us\n",
      "    [7] aten::zero_ --> 186198us\n",
      "    [8] aten::fill_ --> 184042us\n",
      "    [9] autograd::engine::evaluate_function: LogSoftmaxBackward0 --> 93us\n",
      "    [10] LogSoftmaxBackward0 --> 77us\n",
      "    [11] aten::_log_softmax_backward_data --> 59us\n",
      "    [12] autograd::engine::evaluate_function: ViewBackward0 --> 15498us\n",
      "    [13] ViewBackward0 --> 13534us\n",
      "    [14] aten::reshape --> 19364us\n",
      "    [15] aten::_reshape_alias --> 1133us\n",
      "    [16] autograd::engine::evaluate_function: CloneBackward0 --> 621us\n",
      "    [17] CloneBackward0 --> 44us\n",
      "    [18] autograd::engine::evaluate_function: SliceBackward0 --> 283346us\n",
      "    [19] SliceBackward0 --> 282658us\n",
      "    [20] aten::slice_backward --> 282470us\n",
      "    [21] aten::zeros --> 181744us\n",
      "    [22] aten::empty --> 6833us\n",
      "    [23] aten::slice --> 934us\n",
      "    [24] aten::as_strided --> 2444us\n",
      "    [25] aten::copy_ --> 184802us\n",
      "    [26] autograd::engine::evaluate_function: UnsafeViewBackward0 --> 1330us\n",
      "    [27] UnsafeViewBackward0 --> 749us\n",
      "    [28] autograd::engine::evaluate_function: MmBackward0 --> 237us\n",
      "    [29] MmBackward0 --> 212us\n",
      "    [30] aten::t --> 3745us\n",
      "    [31] aten::transpose --> 2871us\n",
      "    [32] aten::mm --> 87553us\n",
      "    [33] autograd::engine::evaluate_function: ReshapeAliasBackward0 --> 23us\n",
      "    [34] ReshapeAliasBackward0 --> 13us\n",
      "    [35] autograd::engine::evaluate_function: TBackward0 --> 1463us\n",
      "    [36] TBackward0 --> 856us\n",
      "    [37] autograd::engine::evaluate_function: NativeLayerNormBackward0 --> 774394us\n",
      "    [38] NativeLayerNormBackward0 --> 727772us\n",
      "    [39] aten::native_layer_norm_backward --> 727420us\n",
      "    [40] autograd::engine::evaluate_function: torch::autograd::AccumulateGrad --> 249577us\n",
      "    [41] torch::autograd::AccumulateGrad --> 140976us\n",
      "    [42] aten::add_ --> 214637us\n",
      "    [43] aten::mul --> 611835us\n",
      "    [44] autograd::engine::evaluate_function: AddBackward0 --> 406us\n",
      "    [45] AddBackward0 --> 49us\n",
      "    [46] autograd::engine::evaluate_function: AddmmBackward0 --> 529575us\n",
      "    [47] AddmmBackward0 --> 90602us\n",
      "    [48] aten::sum --> 436436us\n",
      "    [49] aten::view --> 1982us\n",
      "    [50] c10d::allreduce_ --> 11295us\n",
      "    [51] record_param_comms --> 10478us\n",
      "    [52] autograd::engine::evaluate_function: GeLUFunctionBackward --> 518635us\n",
      "    [53] GeLUFunctionBackward --> 518227us\n",
      "    [54] aten::add --> 25425us\n",
      "    [55] aten::tanh --> 7615us\n",
      "    [56] aten::rsub --> 21371us\n",
      "    [57] aten::sub --> 21265us\n",
      "    [58] autograd::engine::evaluate_function: PermuteBackward0 --> 750us\n",
      "    [59] PermuteBackward0 --> 459us\n",
      "    [60] aten::permute --> 791us\n",
      "    [61] aten::clone --> 21118us\n",
      "    [62] aten::empty_like --> 2291us\n",
      "    [63] aten::_unsafe_view --> 505us\n",
      "    [64] autograd::engine::evaluate_function: BmmBackward0 --> 12897us\n",
      "    [65] BmmBackward0 --> 12566us\n",
      "    [66] aten::bmm --> 32105us\n",
      "    [67] autograd::engine::evaluate_function: SoftmaxBackward0 --> 26714us\n",
      "    [68] SoftmaxBackward0 --> 26464us\n",
      "    [69] aten::_softmax_backward_data --> 26334us\n",
      "    [70] autograd::engine::evaluate_function: MaskedFillBackward0 --> 20989us\n",
      "    [71] MaskedFillBackward0 --> 20752us\n",
      "    [72] aten::masked_fill --> 23150us\n",
      "    [73] aten::expand --> 586us\n",
      "    [74] aten::masked_fill_ --> 16609us\n",
      "    [75] autograd::engine::evaluate_function: BaddbmmBackward0 --> 37812us\n",
      "    [76] BaddbmmBackward0 --> 37430us\n",
      "    [77] autograd::engine::evaluate_function: TransposeBackward0 --> 632us\n",
      "    [78] TransposeBackward0 --> 337us\n",
      "    [79] autograd::engine::evaluate_function: SelectBackward0 --> 82937us\n",
      "    [80] SelectBackward0 --> 59658us\n",
      "    [81] aten::select_backward --> 59479us\n",
      "    [82] aten::select --> 1330us\n",
      "    [83] autograd::engine::evaluate_function: EmbeddingBackward0 --> 698319us\n",
      "    [84] EmbeddingBackward0 --> 698129us\n",
      "    [85] aten::embedding_backward --> 698105us\n",
      "    [86] aten::embedding_dense_backward --> 698092us\n",
      "    [87] aten::arange --> 298us\n",
      "    [88] aten::resize_ --> 38us\n",
      "    [89] torch.distributed.ddp.reducer::copy_bucket_to_grad --> 8122us\n",
      "    [90] aten::to --> 2592us\n",
      "    [91] aten::_to_copy --> 2160us\n",
      "    [92] aten::empty_strided --> 5008us\n",
      "    [93] aten::lift_fresh --> 5us\n",
      "    [94] aten::detach_ --> 12us\n",
      "    [95] detach_ --> 3us\n",
      "    [96] aten::stack --> 212us\n",
      "    [97] aten::cat --> 155us\n",
      "    [98] aten::narrow --> 28us\n",
      "    [99] aten::embedding --> 145us\n",
      "    [100] aten::index_select --> 99us\n",
      "    [101] aten::layer_norm --> 3862us\n",
      "    [102] aten::native_layer_norm --> 3715us\n",
      "    [103] aten::pow --> 55us\n",
      "    [104] aten::cumsum --> 53us\n",
      "    [105] aten::unsqueeze --> 34us\n",
      "    [106] aten::lt --> 47us\n",
      "    [107] aten::bitwise_not --> 44us\n",
      "    [108] aten::__or__ --> 56us\n",
      "    [109] aten::bitwise_or --> 51us\n",
      "    [110] aten::linear --> 11326us\n",
      "    [111] aten::addmm --> 8359us\n",
      "    [112] aten::baddbmm --> 2013us\n",
      "    [113] aten::softmax --> 1058us\n",
      "    [114] aten::_softmax --> 929us\n",
      "    [115] aten::dropout --> 73us\n",
      "    [116] GeLUFunction --> 8239us\n",
      "    [117] aten::matmul --> 141us\n",
      "    [118] aten::contiguous --> 144us\n",
      "    [119] aten::cross_entropy_loss --> 148us\n",
      "    [120] aten::log_softmax --> 63us\n",
      "    [121] aten::_log_softmax --> 59us\n",
      "    [122] aten::nll_loss_nd --> 67us\n",
      "    [123] aten::nll_loss --> 62us\n",
      "    [124] aten::nll_loss_forward --> 54us\n",
      "    [125] aten::ones_like --> 49us\n",
      "    [126] aten::_foreach_add_ --> 5569us\n",
      "    [127] aten::_foreach_mul_ --> 3500us\n",
      "    [128] aten::result_type --> 471us\n",
      "    [129] aten::_foreach_addcmul_ --> 1221us\n",
      "    [130] aten::item --> 123598us\n",
      "    [131] aten::_local_scalar_dense --> 122763us\n",
      "    [132] aten::_foreach_sqrt --> 3614us\n",
      "    [133] aten::_foreach_div_ --> 1124us\n",
      "    [134] aten::_foreach_add --> 3946us\n",
      "    [135] aten::_foreach_addcdiv_ --> 1189us\n",
      "    [+] Total --> 11029661us\n",
      "  user_annotation\n",
      "    [1] nccl:all_reduce --> 7666us\n",
      "    [2] ProfilerStep#1 --> 1750357us\n",
      "    [3] enumerate(DataLoader)#_SingleProcessDataLoaderIter.__next__ --> 3941us\n",
      "    [4] DistributedDataParallel.forward --> 53815us\n",
      "    [5] Optimizer.step#AdamW.step --> 28681us\n",
      "    [6] Optimizer.zero_grad#AdamW.zero_grad --> 8165us\n",
      "    [7] ProfilerStep#2 --> 1750199us\n",
      "    [+] Total --> 3602824us\n",
      "  gpu_memcpy\n",
      "    [1] Memcpy HtoD (Pageable -> Device) --> 34us\n",
      "    [2] Memcpy DtoD (Device -> Device) --> 39246us\n",
      "    [3] Memcpy DtoH (Device -> Pageable) --> 8us\n",
      "    [+] Total --> 39288us\n",
      "  cuda_runtime\n",
      "    [1] cudaMemcpyAsync --> 131685us\n",
      "    [2] cudaStreamSynchronize --> 681563us\n",
      "    [3] cudaEventElapsedTime --> 32us\n",
      "    [4] cudaLaunchKernel --> 2286509us\n",
      "    [5] cudaOccupancyMaxActiveBlocksPerMultiprocessor --> 358us\n",
      "    [6] cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags --> 66us\n",
      "    [7] cudaMemsetAsync --> 207455us\n",
      "    [8] cudaStreamIsCapturing --> 15us\n",
      "    [9] cudaStreamWaitEvent --> 849us\n",
      "    [10] INVALID --> 4us\n",
      "    [11] cudaEventQuery --> 884us\n",
      "    [12] cudaDeviceGetAttribute --> 6us\n",
      "    [13] cudaDeviceSynchronize --> 23us\n",
      "    [+] Total --> 3309449us\n",
      "  kernel\n",
      "    [1] void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long) --> 147us\n",
      "    [2] void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*) --> 3938us\n",
      "    [3] void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}>::result_type*) --> 6us\n",
      "    [4] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2}) --> 10us\n",
      "    [5] std::enable_if<!c10::is_complex<long>::value, void>::type at::native::tensor_kernel_scan_innermost_dim<long, 16, 32, std::plus<long> >(long*, long*, unsigned int, unsigned int, long, std::plus<long>) --> 24us\n",
      "    [6] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>) --> 8us\n",
      "    [7] void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 3>) --> 8us\n",
      "    [8] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#2}) --> 15us\n",
      "    [9] void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*) --> 19us\n",
      "    [10] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1}) --> 10us\n",
      "    [11] void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#11}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#11}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>) --> 10us\n",
      "    [12] void at::native::vectorized_elementwise_kernel<4, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, at::detail::Array<char*, 2> >(int, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, at::detail::Array<char*, 2>) --> 8us\n",
      "    [13] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > const&)::{lambda(int)#1}) --> 14us\n",
      "    [14] ampere_sgemm_128x64_tn --> 680285us\n",
      "    [15] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}) --> 46976us\n",
      "    [16] ampere_sgemm_128x128_nn --> 12144us\n",
      "    [17] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1} const&)::{lambda(int)#1}) --> 4764us\n",
      "    [18] void (anonymous namespace)::softmax_warp_forward<float, float, float, 7, false, false>(float*, float const*, int, int, int, bool const*, int, bool) --> 2220us\n",
      "    [19] ampere_sgemm_128x32_tn --> 54432us\n",
      "    [20] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>) --> 51770us\n",
      "    [21] void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>) --> 62666us\n",
      "    [22] void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>) --> 87982us\n",
      "    [23] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2>) --> 29675us\n",
      "    [24] void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>) --> 13166us\n",
      "    [25] ampere_sgemm_32x128_tn --> 219587us\n",
      "    [26] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}) --> 12us\n",
      "    [27] void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, float, float, at::native::(anonymous namespace)::LogSoftMaxForwardEpilogue>(float*, float*, int) --> 19217us\n",
      "    [28] void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float*, long*, float*, bool, long, long, long, long) --> 290us\n",
      "    [29] void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>) --> 18us\n",
      "    [30] void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>) --> 29161us\n",
      "    [31] void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float*, long*, float*, float*, bool, int, int, long, long) --> 148us\n",
      "    [32] void at::native::(anonymous namespace)::cunn_SoftMaxBackward<4, float, float, float, at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue>(float*, float*, float*, int) --> 18886us\n",
      "    [33] void cutlass::Kernel<cutlass_80_simt_sgemm_32x128_8x5_nt_align1>(cutlass_80_simt_sgemm_32x128_8x5_nt_align1::Params) --> 336085us\n",
      "    [34] ampere_sgemm_64x32_sliced1x4_nn --> 570801us\n",
      "    [35] void at::native::(anonymous namespace)::layer_norm_grad_input_kernel<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int) --> 6789us\n",
      "    [36] void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*) --> 6810us\n",
      "    [37] ampere_sgemm_128x64_nn --> 199635us\n",
      "    [38] void cutlass::Kernel<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params) --> 402734us\n",
      "    [39] void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>) --> 8561us\n",
      "    [40] ncclKernel_AllReduce_RING_LL_Sum_float(ncclDevComm*, unsigned long, ncclWork*) --> 123022us\n",
      "    [41] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2>) --> 5821us\n",
      "    [42] ampere_sgemm_128x32_nn --> 215082us\n",
      "    [43] ampere_sgemm_64x32_sliced1x4_nt --> 52240us\n",
      "    [44] ampere_sgemm_128x128_nt --> 157833us\n",
      "    [45] ampere_sgemm_128x128_tn --> 12212us\n",
      "    [46] void (anonymous namespace)::softmax_warp_backward<float, float, float, 7, false, false>(float*, float const*, float const*, int, int, int, bool const*) --> 2913us\n",
      "    [47] void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, false, long, int>(int*, long const*, int, int, int) --> 44us\n",
      "    [48] void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, int>(int*) --> 13us\n",
      "    [49] void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, false, long, at::cuda::cub::detail::OpaqueType<8>, int, int>(int*, int*, int*, int const*, long*, long const*, at::cuda::cub::detail::OpaqueType<8>*, at::cuda::cub::detail::OpaqueType<8> const*, int, int, int) --> 231us\n",
      "    [50] void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__unique_by_key::InitAgent<at_cuda_detail::cub::ScanTileState<int, true>, int*, int>, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long, int*>(at_cuda_detail::cub::ScanTileState<int, true>, unsigned long, int*) --> 7us\n",
      "    [51] void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__unique_by_key::UniqueByKeyAgent<thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int, int*>, thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int*, int, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long>(thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int*, int, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long) --> 14us\n",
      "    [52] at::native::(anonymous namespace)::write_num_of_segments_for_legacy_thrust_path(long*, long) --> 6us\n",
      "    [53] void at::native::(anonymous namespace)::krn_partials_per_segment<long>(long*, long const*, long*, long) --> 8us\n",
      "    [54] void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, true> >(at_cuda_detail::cub::ScanTileState<long, true>, int) --> 6us\n",
      "    [55] void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long>::Policy600, long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, at::cuda::cub::(anonymous namespace)::SumOp<long>, at_cuda_detail::cub::detail::InputValue<long, long*>, int>(long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, int, at::cuda::cub::(anonymous namespace)::SumOp<long>, at_cuda_detail::cub::detail::InputValue<long, long*>, int) --> 12us\n",
      "    [56] void at::native::(anonymous namespace)::compute_num_of_partial_segments<long>(long*, long*, long*, long*) --> 6us\n",
      "    [57] void at::native::(anonymous namespace)::krn_partial_segment_offset<long>(long*, long const*, long const*, long const*, long*) --> 10us\n",
      "    [58] void at::native::(anonymous namespace)::compute_grad_weight<float, long>(long*, float*, long*, long, long, long*, long*, at::AccumulateType<float, true>::type*, long) --> 118us\n",
      "    [59] void at::native::(anonymous namespace)::sum_and_scatter<float, long>(long*, float*, long, long*, long*, at::AccumulateType<float, true>::type const*, long const*, long*, long, long) --> 82us\n",
      "    [60] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float) --> 34217us\n",
      "    [61] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float) --> 16038us\n",
      "    [62] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<3>, at::native::(anonymous namespace)::PointwiseOpScalarFunctor<float, 3, 3, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<3>, at::native::(anonymous namespace)::PointwiseOpScalarFunctor<float, 3, 3, 0>, std::multiplies<float>, float) --> 16151us\n",
      "    [63] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::UnaryOpFunctor<float, 2, 1, 1>, at::native::Sqrt<float> >(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::UnaryOpFunctor<float, 2, 1, 1>, at::native::Sqrt<float>) --> 11361us\n",
      "    [64] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 1>, at::native::(anonymous namespace)::BinaryOpScalarListFunctor<float, 1, 1, 0>, std::divides<float> >(at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 1>, at::native::(anonymous namespace)::BinaryOpScalarListFunctor<float, 1, 1, 0>, std::divides<float>) --> 11603us\n",
      "    [65] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 2, 1, 1>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 2, 1, 1>, std::plus<float>, float) --> 11258us\n",
      "    [66] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 3>, at::native::(anonymous namespace)::PointwiseOpScalarListFunctor<float, 3, 3, 0>, std::divides<float> >(at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 3>, at::native::(anonymous namespace)::PointwiseOpScalarListFunctor<float, 3, 3, 0>, std::divides<float>) --> 20602us\n",
      "    [+] Total --> 3559941us\n",
      "  gpu_memset\n",
      "    [1] Memset (Device) --> 477us\n",
      "    [+] Total --> 477us\n",
      "  Trace\n",
      "    [1] PyTorch Profiler (0) --> 3500904us\n",
      "    [+] Total --> 3500904us\n",
      "np1-bs32\n",
      "  cpu_op\n",
      "    [1] autograd::engine::evaluate_function: DivBackward0 --> 110us\n",
      "    [2] DivBackward0 --> 89us\n",
      "    [3] aten::div --> 181us\n",
      "    [4] autograd::engine::evaluate_function: NllLossBackward0 --> 202us\n",
      "    [5] NllLossBackward0 --> 169us\n",
      "    [6] aten::nll_loss_backward --> 134us\n",
      "    [7] aten::zero_ --> 91309us\n",
      "    [8] aten::fill_ --> 89228us\n",
      "    [9] autograd::engine::evaluate_function: LogSoftmaxBackward0 --> 97us\n",
      "    [10] LogSoftmaxBackward0 --> 81us\n",
      "    [11] aten::_log_softmax_backward_data --> 60us\n",
      "    [12] autograd::engine::evaluate_function: ViewBackward0 --> 16714us\n",
      "    [13] ViewBackward0 --> 14729us\n",
      "    [14] aten::reshape --> 20505us\n",
      "    [15] aten::_reshape_alias --> 1113us\n",
      "    [16] autograd::engine::evaluate_function: CloneBackward0 --> 626us\n",
      "    [17] CloneBackward0 --> 46us\n",
      "    [18] autograd::engine::evaluate_function: SliceBackward0 --> 92380us\n",
      "    [19] SliceBackward0 --> 91690us\n",
      "    [20] aten::slice_backward --> 91490us\n",
      "    [21] aten::zeros --> 87075us\n",
      "    [22] aten::empty --> 6481us\n",
      "    [23] aten::slice --> 945us\n",
      "    [24] aten::as_strided --> 1813us\n",
      "    [25] aten::copy_ --> 185975us\n",
      "    [26] autograd::engine::evaluate_function: UnsafeViewBackward0 --> 1342us\n",
      "    [27] UnsafeViewBackward0 --> 711us\n",
      "    [28] autograd::engine::evaluate_function: MmBackward0 --> 153us\n",
      "    [29] MmBackward0 --> 132us\n",
      "    [30] aten::t --> 3837us\n",
      "    [31] aten::transpose --> 2984us\n",
      "    [32] aten::mm --> 78080us\n",
      "    [33] autograd::engine::evaluate_function: ReshapeAliasBackward0 --> 17us\n",
      "    [34] ReshapeAliasBackward0 --> 9us\n",
      "    [35] autograd::engine::evaluate_function: TBackward0 --> 1526us\n",
      "    [36] TBackward0 --> 881us\n",
      "    [37] autograd::engine::evaluate_function: NativeLayerNormBackward0 --> 66578us\n",
      "    [38] NativeLayerNormBackward0 --> 44880us\n",
      "    [39] aten::native_layer_norm_backward --> 44559us\n",
      "    [40] autograd::engine::evaluate_function: torch::autograd::AccumulateGrad --> 487902us\n",
      "    [41] torch::autograd::AccumulateGrad --> 485875us\n",
      "    [42] aten::add_ --> 533505us\n",
      "    [43] autograd::engine::evaluate_function: AddBackward0 --> 402us\n",
      "    [44] AddBackward0 --> 38us\n",
      "    [45] autograd::engine::evaluate_function: AddmmBackward0 --> 160819us\n",
      "    [46] AddmmBackward0 --> 81183us\n",
      "    [47] aten::sum --> 77098us\n",
      "    [48] aten::view --> 1993us\n",
      "    [49] autograd::engine::evaluate_function: GeLUFunctionBackward --> 1214051us\n",
      "    [50] GeLUFunctionBackward --> 1213621us\n",
      "    [51] aten::mul --> 1036695us\n",
      "    [52] aten::add --> 136638us\n",
      "    [53] aten::tanh --> 72818us\n",
      "    [54] aten::rsub --> 16003us\n",
      "    [55] aten::sub --> 15911us\n",
      "    [56] autograd::engine::evaluate_function: PermuteBackward0 --> 738us\n",
      "    [57] PermuteBackward0 --> 423us\n",
      "    [58] aten::permute --> 750us\n",
      "    [59] aten::clone --> 119518us\n",
      "    [60] aten::empty_like --> 2225us\n",
      "    [61] aten::_unsafe_view --> 502us\n",
      "    [62] autograd::engine::evaluate_function: BmmBackward0 --> 25797us\n",
      "    [63] BmmBackward0 --> 25442us\n",
      "    [64] aten::bmm --> 46951us\n",
      "    [65] autograd::engine::evaluate_function: SoftmaxBackward0 --> 117370us\n",
      "    [66] SoftmaxBackward0 --> 117102us\n",
      "    [67] aten::_softmax_backward_data --> 116968us\n",
      "    [68] autograd::engine::evaluate_function: MaskedFillBackward0 --> 113527us\n",
      "    [69] MaskedFillBackward0 --> 113267us\n",
      "    [70] aten::masked_fill --> 115623us\n",
      "    [71] aten::expand --> 608us\n",
      "    [72] aten::masked_fill_ --> 11934us\n",
      "    [73] autograd::engine::evaluate_function: BaddbmmBackward0 --> 34902us\n",
      "    [74] BaddbmmBackward0 --> 34505us\n",
      "    [75] autograd::engine::evaluate_function: TransposeBackward0 --> 647us\n",
      "    [76] TransposeBackward0 --> 331us\n",
      "    [77] autograd::engine::evaluate_function: SelectBackward0 --> 88345us\n",
      "    [78] SelectBackward0 --> 65682us\n",
      "    [79] aten::select_backward --> 65487us\n",
      "    [80] aten::select --> 1308us\n",
      "    [81] autograd::engine::evaluate_function: EmbeddingBackward0 --> 804016us\n",
      "    [82] EmbeddingBackward0 --> 803962us\n",
      "    [83] aten::embedding_backward --> 803925us\n",
      "    [84] aten::embedding_dense_backward --> 803913us\n",
      "    [85] aten::arange --> 512us\n",
      "    [86] aten::resize_ --> 40us\n",
      "    [87] aten::to --> 2577us\n",
      "    [88] aten::_to_copy --> 2096us\n",
      "    [89] aten::empty_strided --> 5115us\n",
      "    [90] aten::lift_fresh --> 5us\n",
      "    [91] aten::detach_ --> 15us\n",
      "    [92] detach_ --> 1us\n",
      "    [93] aten::stack --> 235us\n",
      "    [94] aten::cat --> 168us\n",
      "    [95] aten::narrow --> 34us\n",
      "    [96] aten::embedding --> 172us\n",
      "    [97] aten::index_select --> 114us\n",
      "    [98] aten::layer_norm --> 3900us\n",
      "    [99] aten::native_layer_norm --> 3747us\n",
      "    [100] aten::pow --> 64us\n",
      "    [101] aten::cumsum --> 53us\n",
      "    [102] aten::unsqueeze --> 39us\n",
      "    [103] aten::lt --> 54us\n",
      "    [104] aten::bitwise_not --> 47us\n",
      "    [105] aten::__or__ --> 61us\n",
      "    [106] aten::bitwise_or --> 54us\n",
      "    [107] aten::linear --> 11435us\n",
      "    [108] aten::addmm --> 8306us\n",
      "    [109] aten::baddbmm --> 1995us\n",
      "    [110] aten::softmax --> 1054us\n",
      "    [111] aten::_softmax --> 912us\n",
      "    [112] aten::dropout --> 75us\n",
      "    [113] GeLUFunction --> 8087us\n",
      "    [114] aten::matmul --> 150us\n",
      "    [115] aten::contiguous --> 132us\n",
      "    [116] aten::cross_entropy_loss --> 178us\n",
      "    [117] aten::log_softmax --> 69us\n",
      "    [118] aten::_log_softmax --> 56us\n",
      "    [119] aten::nll_loss_nd --> 81us\n",
      "    [120] aten::nll_loss --> 72us\n",
      "    [121] aten::nll_loss_forward --> 61us\n",
      "    [122] aten::ones_like --> 51us\n",
      "    [123] aten::_foreach_add_ --> 5559us\n",
      "    [124] aten::_foreach_mul_ --> 3438us\n",
      "    [125] aten::result_type --> 481us\n",
      "    [126] aten::_foreach_addcmul_ --> 1202us\n",
      "    [127] aten::item --> 103291us\n",
      "    [128] aten::_local_scalar_dense --> 102428us\n",
      "    [129] aten::_foreach_sqrt --> 3672us\n",
      "    [130] aten::_foreach_div_ --> 1098us\n",
      "    [131] aten::_foreach_add --> 3955us\n",
      "    [132] aten::_foreach_addcdiv_ --> 1185us\n",
      "    [+] Total --> 11287377us\n",
      "  user_annotation\n",
      "    [1] ProfilerStep#1 --> 1719507us\n",
      "    [2] enumerate(DataLoader)#_SingleProcessDataLoaderIter.__next__ --> 4080us\n",
      "    [3] Optimizer.step#AdamW.step --> 29030us\n",
      "    [4] Optimizer.zero_grad#AdamW.zero_grad --> 7950us\n",
      "    [5] ProfilerStep#2 --> 1720311us\n",
      "    [+] Total --> 3480878us\n",
      "  gpu_memcpy\n",
      "    [1] Memcpy HtoD (Pageable -> Device) --> 30us\n",
      "    [2] Memcpy DtoD (Device -> Device) --> 14443us\n",
      "    [3] Memcpy DtoH (Device -> Pageable) --> 8us\n",
      "    [+] Total --> 14481us\n",
      "  cuda_runtime\n",
      "    [1] cudaMemcpyAsync --> 203881us\n",
      "    [2] cudaStreamSynchronize --> 772533us\n",
      "    [3] cudaLaunchKernel --> 2248392us\n",
      "    [4] cudaOccupancyMaxActiveBlocksPerMultiprocessor --> 329us\n",
      "    [5] cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags --> 58us\n",
      "    [6] cudaMemsetAsync --> 51331us\n",
      "    [7] cudaDeviceGetAttribute --> 5us\n",
      "    [8] cudaStreamIsCapturing --> 8us\n",
      "    [9] cudaDeviceSynchronize --> 14us\n",
      "    [+] Total --> 3276551us\n",
      "  kernel\n",
      "    [1] void at::native::(anonymous namespace)::indexSelectLargeIndex<float, long, unsigned int, 2, 2, -2, true>(at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<float, unsigned int>, at::cuda::detail::TensorInfo<long, unsigned int>, int, int, unsigned int, unsigned int, long) --> 148us\n",
      "    [2] void at::native::(anonymous namespace)::vectorized_layer_norm_kernel<float, float>(int, float, float const*, float const*, float const*, float*, float*, float*) --> 3964us\n",
      "    [3] void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#3}::operator()() const::{lambda(long)#1}>::result_type*) --> 6us\n",
      "    [4] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::pow_tensor_tensor_kernel(at::TensorIteratorBase&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, float)#1} const&)::{lambda(int)#2}) --> 10us\n",
      "    [5] std::enable_if<!c10::is_complex<long>::value, void>::type at::native::tensor_kernel_scan_innermost_dim<long, 16, 32, std::plus<long> >(long*, long*, unsigned int, unsigned int, long, std::plus<long>) --> 23us\n",
      "    [6] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<long>, at::detail::Array<char*, 2>) --> 8us\n",
      "    [7] void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<long, long, long, at::native::binary_internal::MulFunctor<long> >, at::detail::Array<char*, 3>) --> 8us\n",
      "    [8] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#2}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> > const&)::{lambda(int)#2}) --> 14us\n",
      "    [9] void (anonymous namespace)::elementwise_kernel_with_index<int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(int, at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}, function_traits<at::native::arange_cuda_out(c10::Scalar const&, c10::Scalar const&, c10::Scalar const&, at::Tensor&)::{lambda()#1}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>::result_type*) --> 12us\n",
      "    [10] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::CompareFunctor<long> >(at::TensorIteratorBase&, at::native::(anonymous namespace)::CompareFunctor<long> const&)::{lambda(int)#1}) --> 10us\n",
      "    [11] void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#11}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1> >(int, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#11}::operator()() const::{lambda(bool)#1}, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int>, TrivialOffsetCalculator<1, unsigned int>, at::native::memory::LoadWithCast<1>, at::native::memory::StoreWithCast<1>) --> 10us\n",
      "    [12] void at::native::vectorized_elementwise_kernel<4, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, at::detail::Array<char*, 2> >(int, at::native::bitwise_not_kernel_cuda(at::TensorIteratorBase&)::{lambda(bool)#1}, at::detail::Array<char*, 2>) --> 7us\n",
      "    [13] void at::native::elementwise_kernel<128, 4, at::native::gpu_kernel_impl<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > >(at::TensorIteratorBase&, at::native::BinaryFunctor<bool, bool, bool, at::native::BitwiseOrFunctor<bool> > const&)::{lambda(int)#1}) --> 14us\n",
      "    [14] ampere_sgemm_128x64_tn --> 680605us\n",
      "    [15] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#7}::operator()() const::{lambda(float)#1} const&)::{lambda(int)#1}) --> 46910us\n",
      "    [16] ampere_sgemm_128x128_nn --> 12143us\n",
      "    [17] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1}>(at::TensorIteratorBase&, at::native::(anonymous namespace)::masked_fill_kernel<bool>(at::TensorIterator&, c10::Scalar const&)::{lambda()#1}::operator()() const::{lambda()#7}::operator()() const::{lambda(float, bool)#1} const&)::{lambda(int)#1}) --> 4768us\n",
      "    [18] void (anonymous namespace)::softmax_warp_forward<float, float, float, 7, false, false>(float*, float const*, int, int, int, bool const*, int, bool) --> 2217us\n",
      "    [19] ampere_sgemm_128x32_tn --> 54426us\n",
      "    [20] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3> >(int, at::native::CUDAFunctor_add<float>, at::detail::Array<char*, 3>) --> 48548us\n",
      "    [21] void at::native::vectorized_elementwise_kernel<4, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::AUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>) --> 47659us\n",
      "    [22] void at::native::vectorized_elementwise_kernel<4, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3> >(int, at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 3>) --> 84290us\n",
      "    [23] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnSelf_add<float>, at::detail::Array<char*, 2>) --> 28076us\n",
      "    [24] void at::native::vectorized_elementwise_kernel<4, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2> >(int, at::native::tanh_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#2}::operator()() const::{lambda(float)#1}, at::detail::Array<char*, 2>) --> 11339us\n",
      "    [25] ampere_sgemm_32x128_tn --> 219767us\n",
      "    [26] void at::native::elementwise_kernel<128, 2, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}>(int, at::native::gpu_kernel_impl<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1}>(at::TensorIteratorBase&, at::native::direct_copy_kernel_cuda(at::TensorIteratorBase&)::{lambda()#2}::operator()() const::{lambda()#4}::operator()() const::{lambda(long)#1} const&)::{lambda(int)#1}) --> 12us\n",
      "    [27] void at::native::(anonymous namespace)::cunn_SoftMaxForward<4, float, float, float, at::native::(anonymous namespace)::LogSoftMaxForwardEpilogue>(float*, float*, int) --> 19216us\n",
      "    [28] void at::native::(anonymous namespace)::nll_loss_forward_reduce_cuda_kernel_2d<float, float, long>(float*, float*, float*, long*, float*, bool, long, long, long, long) --> 291us\n",
      "    [29] void at::native::vectorized_elementwise_kernel<4, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2> >(int, at::native::BUnaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float> >, at::detail::Array<char*, 2>) --> 18us\n",
      "    [30] void at::native::vectorized_elementwise_kernel<4, at::native::FillFunctor<float>, at::detail::Array<char*, 1> >(int, at::native::FillFunctor<float>, at::detail::Array<char*, 1>) --> 29095us\n",
      "    [31] void at::native::(anonymous namespace)::nll_loss_backward_reduce_cuda_kernel_2d<float, long>(float*, float*, long*, float*, float*, bool, int, int, long, long) --> 147us\n",
      "    [32] void at::native::(anonymous namespace)::cunn_SoftMaxBackward<4, float, float, float, at::native::(anonymous namespace)::LogSoftMaxBackwardEpilogue>(float*, float*, float*, int) --> 18876us\n",
      "    [33] void cutlass::Kernel<cutlass_80_simt_sgemm_32x128_8x5_nt_align1>(cutlass_80_simt_sgemm_32x128_8x5_nt_align1::Params) --> 336107us\n",
      "    [34] ampere_sgemm_64x32_sliced1x4_nn --> 570319us\n",
      "    [35] void at::native::(anonymous namespace)::layer_norm_grad_input_kernel<float, float>(float const*, float const*, float const*, float const*, float const*, float*, int) --> 6036us\n",
      "    [36] void at::native::(anonymous namespace)::GammaBetaBackwardCUDAKernel_32x32<float, float>(long, long, float const*, float const*, float const*, float const*, float*, float*) --> 4710us\n",
      "    [37] ampere_sgemm_128x64_nn --> 199566us\n",
      "    [38] void cutlass::Kernel<cutlass_80_simt_sgemm_128x32_8x5_nt_align1>(cutlass_80_simt_sgemm_128x32_8x5_nt_align1::Params) --> 402665us\n",
      "    [39] void at::native::reduce_kernel<128, 4, at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4> >(at::native::ReduceOp<float, at::native::func_wrapper_t<float, at::native::sum_functor<float, float, float>::operator()(at::TensorIterator&)::{lambda(float, float)#1}>, unsigned int, float, 4>) --> 8518us\n",
      "    [40] void at::native::vectorized_elementwise_kernel<4, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2> >(int, at::native::CUDAFunctorOnOther_add<float>, at::detail::Array<char*, 2>) --> 5778us\n",
      "    [41] ampere_sgemm_128x32_nn --> 213859us\n",
      "    [42] ampere_sgemm_64x32_sliced1x4_nt --> 52219us\n",
      "    [43] ampere_sgemm_128x128_nt --> 157779us\n",
      "    [44] ampere_sgemm_128x128_tn --> 12205us\n",
      "    [45] void (anonymous namespace)::softmax_warp_backward<float, float, float, 7, false, false>(float*, float const*, float const*, int, int, int, bool const*) --> 2912us\n",
      "    [46] void at_cuda_detail::cub::DeviceRadixSortHistogramKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, false, long, int>(int*, long const*, int, int, int) --> 13us\n",
      "    [47] void at_cuda_detail::cub::DeviceRadixSortExclusiveSumKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, int>(int*) --> 8us\n",
      "    [48] void at_cuda_detail::cub::DeviceRadixSortOnesweepKernel<at_cuda_detail::cub::DeviceRadixSortPolicy<long, at::cuda::cub::detail::OpaqueType<8>, int>::Policy800, false, long, at::cuda::cub::detail::OpaqueType<8>, int, int>(int*, int*, int*, int const*, long*, long const*, at::cuda::cub::detail::OpaqueType<8>*, at::cuda::cub::detail::OpaqueType<8> const*, int, int, int) --> 172us\n",
      "    [49] void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__unique_by_key::InitAgent<at_cuda_detail::cub::ScanTileState<int, true>, int*, int>, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long, int*>(at_cuda_detail::cub::ScanTileState<int, true>, unsigned long, int*) --> 8us\n",
      "    [50] void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__unique_by_key::UniqueByKeyAgent<thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int, int*>, thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int*, int, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long>(thrust::device_ptr<long>, thrust::counting_iterator<int, thrust::use_default, thrust::use_default, thrust::use_default>, thrust::device_ptr<long>, thrust::device_ptr<long>, thrust::equal_to<long>, int*, int, at_cuda_detail::cub::ScanTileState<int, true>, unsigned long) --> 14us\n",
      "    [51] at::native::(anonymous namespace)::write_num_of_segments_for_legacy_thrust_path(long*, long) --> 6us\n",
      "    [52] void at::native::(anonymous namespace)::krn_partials_per_segment<long>(long*, long const*, long*, long) --> 8us\n",
      "    [53] void at_cuda_detail::cub::DeviceScanInitKernel<at_cuda_detail::cub::ScanTileState<long, true> >(at_cuda_detail::cub::ScanTileState<long, true>, int) --> 6us\n",
      "    [54] void at_cuda_detail::cub::DeviceScanKernel<at_cuda_detail::cub::DeviceScanPolicy<long>::Policy600, long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, at::cuda::cub::(anonymous namespace)::SumOp<long>, at_cuda_detail::cub::detail::InputValue<long, long*>, int>(long const*, long*, at_cuda_detail::cub::ScanTileState<long, true>, int, at::cuda::cub::(anonymous namespace)::SumOp<long>, at_cuda_detail::cub::detail::InputValue<long, long*>, int) --> 10us\n",
      "    [55] void at::native::(anonymous namespace)::compute_num_of_partial_segments<long>(long*, long*, long*, long*) --> 6us\n",
      "    [56] void at::native::(anonymous namespace)::krn_partial_segment_offset<long>(long*, long const*, long const*, long const*, long*) --> 10us\n",
      "    [57] void at::native::(anonymous namespace)::compute_grad_weight<float, long>(long*, float*, long*, long, long, long*, long*, at::AccumulateType<float, true>::type*, long) --> 118us\n",
      "    [58] void at::native::(anonymous namespace)::sum_and_scatter<float, long>(long*, float*, long, long*, long*, at::AccumulateType<float, true>::type const*, long const*, long*, long, long) --> 83us\n",
      "    [59] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<1>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 1, 1, 0>, std::multiplies<float>, float) --> 34157us\n",
      "    [60] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpListAlphaFunctor<float, 2, 2, 0>, std::plus<float>, float) --> 15976us\n",
      "    [61] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<3>, at::native::(anonymous namespace)::PointwiseOpScalarFunctor<float, 3, 3, 0>, std::multiplies<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<3>, at::native::(anonymous namespace)::PointwiseOpScalarFunctor<float, 3, 3, 0>, std::multiplies<float>, float) --> 16150us\n",
      "    [62] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::UnaryOpFunctor<float, 2, 1, 1>, at::native::Sqrt<float> >(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::UnaryOpFunctor<float, 2, 1, 1>, at::native::Sqrt<float>) --> 11334us\n",
      "    [63] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 1>, at::native::(anonymous namespace)::BinaryOpScalarListFunctor<float, 1, 1, 0>, std::divides<float> >(at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 1>, at::native::(anonymous namespace)::BinaryOpScalarListFunctor<float, 1, 1, 0>, std::divides<float>) --> 11566us\n",
      "    [64] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 2, 1, 1>, std::plus<float>, float>(at::native::(anonymous namespace)::TensorListMetadata<2>, at::native::(anonymous namespace)::BinaryOpScalarFunctor<float, 2, 1, 1>, std::plus<float>, float) --> 11225us\n",
      "    [65] void at::native::(anonymous namespace)::multi_tensor_apply_kernel<at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 3>, at::native::(anonymous namespace)::PointwiseOpScalarListFunctor<float, 3, 3, 0>, std::divides<float> >(at::native::(anonymous namespace)::TensorListScalarListMetadata<float, 3>, at::native::(anonymous namespace)::PointwiseOpScalarListFunctor<float, 3, 3, 0>, std::divides<float>) --> 20650us\n",
      "    [+] Total --> 3406830us\n",
      "  gpu_memset\n",
      "    [1] Memset (Device) --> 426us\n",
      "    [+] Total --> 426us\n",
      "  Trace\n",
      "    [1] PyTorch Profiler (0) --> 3440147us\n",
      "    [+] Total --> 3440147us\n"
     ]
    }
   ],
   "source": [
    "traces = glob(\"./logs/bigscience/bloom-1b1/**/*.json\", recursive=True)\n",
    "total_durs = {}\n",
    "\n",
    "for trace in traces:\n",
    "    with open(trace) as f:\n",
    "        json_dict = json.load(f)\n",
    "\n",
    "    total_dur = {}\n",
    "\n",
    "    for event in json_dict[\"traceEvents\"]:\n",
    "        if not \"dur\" in event:\n",
    "            continue\n",
    "\n",
    "        cat = event[\"cat\"]\n",
    "        dur = event[\"dur\"]\n",
    "        name = event[\"name\"]\n",
    "\n",
    "        if not dur:\n",
    "            continue\n",
    "\n",
    "        if not cat in total_dur:\n",
    "            total_dur[cat] = defaultdict(int)\n",
    "\n",
    "        # if \"aten::\" in name:\n",
    "        #     name = \"ManipulateTensor\"\n",
    "        # elif \"autograd::\" in name:\n",
    "        #     name = \"CalculateGradient\"\n",
    "\n",
    "        total_dur[cat][name] += dur\n",
    "\n",
    "    dirname = trace.split(\"/\")[-2]\n",
    "    total_durs[dirname] = total_dur\n",
    "    print(dirname)\n",
    "\n",
    "    for cat, names in total_dur.items():\n",
    "        cat_dur = 0\n",
    "        print(f\"  {cat}\")\n",
    "        for idx, (name, dur) in enumerate(names.items()):\n",
    "            print(f\"    [{idx+1}] {name} --> {dur}us\")\n",
    "            cat_dur += dur\n",
    "        print(f\"    [+] Total --> {cat_dur}us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autograd::engine::evaluate_function: DivBackward0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DivBackward0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aten::div</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>autograd::engine::evaluate_function: NllLossBa...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NllLossBackward0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>aten::_local_scalar_dense</td>\n",
       "      <td>102428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>aten::_foreach_sqrt</td>\n",
       "      <td>3672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>aten::_foreach_div_</td>\n",
       "      <td>1098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>aten::_foreach_add</td>\n",
       "      <td>3955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>aten::_foreach_addcdiv_</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name     dur\n",
       "0    autograd::engine::evaluate_function: DivBackward0     110\n",
       "1                                         DivBackward0      89\n",
       "2                                            aten::div     181\n",
       "3    autograd::engine::evaluate_function: NllLossBa...     202\n",
       "4                                     NllLossBackward0     169\n",
       "..                                                 ...     ...\n",
       "127                          aten::_local_scalar_dense  102428\n",
       "128                                aten::_foreach_sqrt    3672\n",
       "129                                aten::_foreach_div_    1098\n",
       "130                                 aten::_foreach_add    3955\n",
       "131                            aten::_foreach_addcdiv_    1185\n",
       "\n",
       "[132 rows x 2 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(total_durs[\"np1-bs32\"][\"cpu_op\"].items())\n",
    "df.columns = [\"name\", \"dur\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ManipulateTensor (length: 132 --> 49)\n",
      "CalculateGradient (length: 49 --> 26)\n",
      "Backward (length: 26 --> 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>detach_</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GeLUFunction</td>\n",
       "      <td>8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ManipulateTensor</td>\n",
       "      <td>4956180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CalculateGradient</td>\n",
       "      <td>3714136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Backward</td>\n",
       "      <td>2608973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name      dur\n",
       "0            detach_        1\n",
       "1       GeLUFunction     8087\n",
       "2   ManipulateTensor  4956180\n",
       "3  CalculateGradient  3714136\n",
       "4           Backward  2608973"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = {\"aten::\": \"ManipulateTensor\", \"autograd::\": \"CalculateGradient\", \"Backward\": \"Backward\"}\n",
    "for target, alt_name in targets.items():\n",
    "    len_ori = len(df)\n",
    "    df_temp = df[df[\"name\"].str.contains(target)]\n",
    "    df.loc[len_ori] = ({\"name\": alt_name, \"dur\": df_temp[\"dur\"].sum()})\n",
    "    df.drop(df_temp.index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    print(f\"{alt_name} (length: {len_ori} --> {len(df)})\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
